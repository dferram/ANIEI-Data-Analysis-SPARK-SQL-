```markdown
# ANIEI-Data-Analysis-SPARK-SQL
¬°Bienvenido! üöÄ  
Este repositorio contiene un an√°lisis de datos de ventas realizado con PySpark y visualizaciones finales con matplotlib y seaborn. El objetivo es demostrar c√≥mo cargar, limpiar, transformar y analizar un dataset de ventas del mundo real usando Spark SQL para obtener insights accionables (ventas por producto, tendencia temporal, top clientes, etc.).

Descripci√≥n corta
-----------------
El notebook procesa un conjunto de datos de ventas, aplica transformaciones y agrega informaci√≥n con SQL y DataFrame API de PySpark. Despu√©s realiza an√°lisis exploratorio y produce visualizaciones que ayudan a entender patrones de ventas por producto, categor√≠a, regi√≥n y tiempo.

Caracter√≠sticas principales
--------------------------
- Lectura eficiente de grandes archivos CSV con PySpark.
- Limpieza y normalizaci√≥n de campos (fechas, nulos, tipos).
- C√°lculo de m√©tricas derivadas (precio total, m√°rgenes, unidades por pedido).
- Consultas SQL sobre vistas temporales para extraer m√©tricas de negocio.
- Agregaciones y ventanas para identificar top-N productos/clientes.
- Series temporales mensuales y an√°lisis de estacionalidad.
- Exportaci√≥n de resultados a Parquet/CSV y gr√°ficos con matplotlib/seaborn.

Requisitos
----------
- Python 3.8+  
- Apache Spark compatible con PySpark (recomendado 3.x)  
- Paquetes Python: pyspark, pandas, matplotlib, seaborn, jupyter (o jupyterlab)  
- Opcional: entorno con m√°s memoria si trabaja con datasets grandes (Spark local config)

Instalaci√≥n r√°pida
------------------
1. Crear entorno virtual:
   python -m venv .venv && source .venv/bin/activate
2. Instalar dependencias:
   pip install pyspark pandas matplotlib seaborn jupyter

C√≥mo usar
--------
- Abrir y ejecutar el notebook en Jupyter:
  jupyter notebook ANIEI_Data_Analysis.ipynb
- O ejecutar transformaciones por l√≠nea con spark-submit si hay scripts .py:
  spark-submit --master local[*] scripts/analyze_sales.py

Estructura del repositorio
--------------------------
- ANIEI_Data_Analysis.ipynb  ‚Äî Notebook principal con todo el flujo.
- data/                      ‚Äî Carpeta esperada para los CSV de entrada (no incluida).
- outputs/                   ‚Äî Carpeta para resultados exportados (parquet/csv/png).
- scripts/                   ‚Äî Scripts auxiliares (si existen).

Flujo de procesamiento (paso a paso)
-----------------------------------
A continuaci√≥n se explica detalladamente lo que hace el c√≥digo, paso a paso:

1) Inicializaci√≥n de Spark
   - Se crea un SparkSession con configuraciones iniciales (memoria, paralelismo).
   - Ejemplo: SparkSession.builder.appName("SalesAnalysis").getOrCreate()

2) Carga de datos
   - Lectura de archivos CSV con opciones: header=True, inferSchema=False (preferible definir schema).
   - Soporta particionado y lectura desde rutas locales o HDFS/S3.
   - Se cargan tambi√©n tablas lookup opcionales (categor√≠as, regiones).

3) Definici√≥n y aplicaci√≥n de esquema
   - Se define un esquema expl√≠cito para forzar tipos correctos (date, int, double, string).
   - Conversi√≥n de columnas de fecha con to_date / unix_timestamp cuando sea necesario.

4) Limpieza y normalizaci√≥n
   - Eliminaci√≥n o imputaci√≥n de nulos en columnas cr√≠ticas (cantidad, precio, fecha).
   - Eliminaci√≥n de duplicados.
   - Normalizaci√≥n de texto (trim, lower) para categor√≠as y nombres de producto.
   - Correcciones b√°sicas (valores negativos en cantidad o precio).

5) Enriquecimiento y columnas derivadas
   - Creaci√≥n de columnas calculadas: total_sale = quantity * unit_price.
   - Extracci√≥n de componentes temporales: year, month, day, year_month.
   - Mapeo de c√≥digos a nombres usando joins con tablas de referencia (si existen).

6) Registro de vistas temporales (Spark SQL)
   - Las tablas transformadas se registran como vistas temporales:
     df.createOrReplaceTempView("sales")
   - Permite ejecutar consultas SQL para an√°lisis ad-hoc y reportes.

7) Agregaciones y consultas SQL
   - Ventas por categor√≠a / producto / regi√≥n:
     SELECT category, SUM(total_sale) FROM sales GROUP BY category ORDER BY SUM(total_sale) DESC
   - Top N productos por ventas usando window functions:
     ROW_NUMBER() OVER (PARTITION BY category ORDER BY total_sale DESC)
   - Tendencias temporales: ventas mensuales, crecimiento a√±o a a√±o (YoY), promedio m√≥vil.

8) An√°lisis avanzado
   - Identificaci√≥n de clientes m√°s valiosos (RFM / recency-frequency-monetary simple).
   - Detecci√≥n de estacionalidad y picos en series temporales.
   - Pivot tables (por ejemplo, ventas por mes por categor√≠a) con pivot() o SQL.

9) Guardado de resultados
   - Exportar tablas finales a Parquet para uso posterior:
     df.write.mode("overwrite").parquet("outputs/sales_aggregated.parquet")
   - Opcional export to CSV para compartir con stakeholders.

10) Visualizaci√≥n
   - Conversi√≥n de peque√±as muestras o agregaciones a pandas (toPandas) para graficar.
   - Gr√°ficas incluidas: barras (top productos), series temporales (ventas por mes), heatmaps (categor√≠a vs mes).
   - Uso de seaborn para estilos y matplotlib para ajustes finales.

11) Conclusiones y recomendaciones
   - El notebook sugiere insights (ej. categor√≠as que impulsan la facturaci√≥n, productos con ca√≠da en ventas).
   - Recomendaciones t√≠picas: revisar pricing, campa√±as estacionales, optimizar inventario en top-N productos.

Buenas pr√°cticas y rendimiento
------------------------------
- Defina esquemas en la lectura para evitar inferencia costosa.
- Use persist/caching en DataFrames reutilizados varias veces.
- Particione la salida Parquet por year_month si har√° consultas por rango de fecha.
- Evite toPandas() sobre conjuntos grandes; convierta solo las agregaciones peque√±as.

Ejemplos de consultas √∫tiles
----------------------------
- Ventas totales por a√±o:
  SELECT year, SUM(total_sale) AS total FROM sales GROUP BY year ORDER BY year
- Top 10 clientes por ingresos:
  SELECT customer_id, SUM(total_sale) AS revenue FROM sales GROUP BY customer_id ORDER BY revenue DESC LIMIT 10

Qu√© esperar como resultado
--------------------------
- Tablas agregadas con m√©tricas de negocio (ventas totales, unidades vendidas).
- Visualizaciones PNG/figuras dentro del notebook.
- Archivos Parquet/CSV con resultados listos para reporting.

Contribuciones y contacto
-------------------------
Si quieres mejorar el an√°lisis (m√°s KPIs, integraciones, pipelines), abre un issue o PR.  
Autor: ferramdr  
Contacto: ver perfil en GitHub

Licencia
--------
Incluye la licencia que prefieras (MIT, Apache-2.0, etc.) ‚Äî actualmente no hay una licencia expl√≠cita en el repo.

¬°Listo! Este README te da una gu√≠a completa para entender y ejecutar el an√°lisis en el notebook. Si quieres, puedo:
- Generar snippets concretos de Spark SQL o PySpark para cada paso.
- A√±adir instrucciones para desplegar en Databricks/AWS EMR.
- Crear un script spark-submit listo para producci√≥n.
Dime cu√°l prefieres y lo preparo.
```
